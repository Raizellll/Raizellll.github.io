---
layout: page
title: "Research"
permalink: /research/
---

## Research

### Expert Subnetwork Discovery in Transformer Models  
*Visiting Scholar, University of Michigan — with Prof. Robert Dick (2025– )*

We study **latent “expert” modularity** inside pretrained Transformers by clustering activation trajectories across layers.

- Identify early vs. late layer specialization  
- Analyze representation drift using CKA  
- Evaluate expert subnetworks for reasoning tasks  
- Study efficiency–accuracy trade-offs

---

### Neural Activation Diagnosis (NAD) & MUI-Rubrics  
*Fudan University — with Prof. Yixin Cao (2025– )*

We aim to build **activation-based evaluators** that map neural subspaces to human-like rubrics.

- Discover interpretable activation subspaces  
- Design rubric-aligned dimensions (process, coherence, creativity)  
- Prototype activation-based evaluators

---

### Long-Form Generation Evaluation (LongWriter × HelloBench × WritingBench)  
*Fudan University (2024–2025)*

- Large-scale LongWriter inference  
- Evaluate S_l and S_q metrics  
- Compare human+LLM vs LLM-only judging  
- Identify systematic gaps in reasoning/coherence/creativity
